[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ahmad Alamiri",
    "section": "",
    "text": "Applications Scientist with interests (mostly) in science, mathematics, and technology."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html",
    "title": "Scripting Node Guide",
    "section": "",
    "text": "This is a guide for developing Python scripts to be used with Thermo Fisher Scientific Compound Discoverer 3.3 SP3 (CD 3.3.3) software via the Scripting Node feature. This guide has been tested with limited scope in CD 3.3.3 primarily using the GC Workflows. It is assumed that the user has already read and understood the Scripting Node - Custom Script Integration section of the Compound Discoverer (or Proteome Discoverer) User Guide.\nThe objectives of this guide are as follows:\n\nDemonstrate how to read (load) data previously exported from Compound Discoverer into a Python environment to perform calculations and computations outside of Compound Discoverer.\nDemonstrate how to import data back into Compound Discoverer (after having performed the desired calculations) and how to display the new data in the form of a new column, a new table, or both.\nClarify the mechanism of the data export out of and import into Compound Discoverer as well as the structures of the JSON files associated with these processes and how to read, modify, and write them."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#introduction",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#introduction",
    "title": "Scripting Node Guide",
    "section": "",
    "text": "This is a guide for developing Python scripts to be used with Thermo Fisher Scientific Compound Discoverer 3.3 SP3 (CD 3.3.3) software via the Scripting Node feature. This guide has been tested with limited scope in CD 3.3.3 primarily using the GC Workflows. It is assumed that the user has already read and understood the Scripting Node - Custom Script Integration section of the Compound Discoverer (or Proteome Discoverer) User Guide.\nThe objectives of this guide are as follows:\n\nDemonstrate how to read (load) data previously exported from Compound Discoverer into a Python environment to perform calculations and computations outside of Compound Discoverer.\nDemonstrate how to import data back into Compound Discoverer (after having performed the desired calculations) and how to display the new data in the form of a new column, a new table, or both.\nClarify the mechanism of the data export out of and import into Compound Discoverer as well as the structures of the JSON files associated with these processes and how to read, modify, and write them."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#data-files",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#data-files",
    "title": "Scripting Node Guide",
    "section": "Data Files",
    "text": "Data Files\nThis guide uses the Oregano Study example data files referenced in the Compound Discoverer GC EI/MS Tutorial PDF (Software Version 3.3 SP3). The example data files are typically provided on a USB drive that is included as part of the software media kit. The files can be found in the following folder: ‘Example Studies/GC/EI/Oregano’."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#scripting-node-overview",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#scripting-node-overview",
    "title": "Scripting Node Guide",
    "section": "Scripting Node Overview",
    "text": "Scripting Node Overview\nTo use the Scripting Node feature, the user declares desired tables and/or columns to be exported using a script written in his/her preferred programming language. To create a script for the Scripting Node, programmers can use any programming language that supports running code from the command line, for example, Python, R, C#, C++, and so on. The desired objects are then exported from Compound Discoverer in the form of tab-separated text files along with JSON files describing the location(s) and structure(s) of these files. Using the script, the user can opt to perform data analysis and calculations outside of Compound Discoverer (with no intention of importing the data back into the application). Alternatively, using the script and following data analysis, the user can opt to import the data back into Compound Discoverer and save it as part of a typical Result file. The import process requires the user to submit the modified data in the form of text files along with JSON files describing the location(s) and structure(s) of these files. This guide will cover all of these scenarios."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#install-libraries-modules-and-packages",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#install-libraries-modules-and-packages",
    "title": "Scripting Node Guide",
    "section": "Install Libraries, Modules, and Packages",
    "text": "Install Libraries, Modules, and Packages\nLoad all libraries, modules, and packages required by the script. At minimum, load a library that is capable of reading JSON files. In this guide, we will use the module ‘JSON’. Additionally, we will load the following libraries to help debug and diagnose any errors or warnings that we may encounter during the script execution process. While Compound Discoverer does provide some traceback information, it may not be as verbose as one might like.\n\nimport json    # JSON encoder and decoder.\nimport os    # Miscellaneous operating system interfaces.\nimport pandas as pd  # Pandas is a Python library for data analysis and manipulation.\nimport sys    # System-specific parameters and functions.\nimport traceback    # Print or retrieve a stack traceback."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#read-command-line-arguments",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#read-command-line-arguments",
    "title": "Scripting Node Guide",
    "section": "Read Command Line Arguments",
    "text": "Read Command Line Arguments\nIn this section, we will read the Command Line arguments passed by Compound Discoverer upon initiation of the scripting node feature. We are only interested in the 2nd ([1]) argument, which contains the location of the would be newly-created node_args.json file. This file contains essential information about the exported data, including location(s) of the exported text files as well as the row IDs, columns, and column attributes of the exported tables.\nDefine Input File (node_args.json file) - located in the 2nd ([1]) argument of the object sys.argv.\n\ninput_file = sys.argv[1]\n\nRead node_args.json file. We will also include an error handling condition as a reference marker for this portion of the script.\n\ntry:\n    with open(input_file, mode='r') as f:\n        node_args = json.load(f)\n    print('Successfully read Compound Discoverer node_args.json file!')\n\nexcept Exception as e:\n    print('Failed to read Compound Discoverer node_args.json file: ' + str(e))\n    print(traceback.format_exc())\n    exit(1)"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#debugging-and-development-optional",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#debugging-and-development-optional",
    "title": "Scripting Node Guide",
    "section": "Debugging and Development (Optional)",
    "text": "Debugging and Development (Optional)\nIn this section, we will save the results of the script (until this point). While this step may not be necessary at this time (since we have not actually loaded any data), it can be quite useful for familiarization with the exported data as well as script development. Nevertheless, we can perform this step at any desired moment in time throughout the script. Once the user is more familiar with the exported data and their structure(s), he/she can opt to include this section as the final step of a script for further development and troubleshooting (as needed).\nSave data (as JSON file). This will save the script_data.json file in the same directory location as the script used in the Scripting Node - adjust file save location as desired.\nFirst define the script data objects to be saved.\n\nscript_data = {\n    'sys_argv': sys.argv,\n    'input_file': input_file,\n    'node_args': node_args\n    }\n\nDefine Output File name, directory, and path. This will save the ‘script_data.json’ file in the same location as the script used in the Scripting Node - adjust file save location as desired.\n\nscript_data_outfilename = 'script_data.json'\ndirectory = os.path.dirname(sys.argv[0])\nscript_data_outfilepath = os.path.join(directory, script_data_outfilename)\n\nFinally, save the data.\n\ntry:\n    with open(script_data_outfilepath, mode='w') as f:\n        json.dump(script_data, f)\n    print('Successfully saved script_data.json file!')\n\nexcept Exception as e:\n    print('Failed to save script_data.json file: ' + str(e))\n    print(traceback.format_exc())\n    exit(1)\n\nIf needed, the user can use the command below to load the saved script_data.json file for debugging or script development purposes. Uncomment and adjust the command to reflect the actual file location. Be sure to adjust the current Working Directory. Alternatively, point to the path of the saved file.\n\nworking_directory = os.path.dirname(sys.argv[0])\nos.chdir(working_directory)\n\nLoad the saved ‘script_data.json’ file for debugging or script development purposes.\n\ntry:\n    with open('script_data.json', mode='r') as f:\n        script_data = json.load(f)\n    print('Successfully read script_data.json file!')\n\nexcept Exception as e:\n    print('Failed to read script_data.json file: ' + str(e))\n    print(traceback.format_exc())\n    exit(1)"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#define-templates",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#define-templates",
    "title": "Scripting Node Guide",
    "section": "Define Templates",
    "text": "Define Templates\n\nNode files, tables, and columns\nIn general, these templates are not necessary, but may serve a purpose in better understanding the mechanism of data export out of Compound Discoverer (and, if needed, data import back into Compound Discoverer) as well as the structures of the associated node_args.json and node_response.json files.\nThe node_args.json file is used to describe the structures and locations of the data files exported out of Compound Discoverer. The node_response.json file is used to describe the structures and locations of the data files being imported back into Compound Discoverer.\nBelow is the structure used by both node_args.json and node_response.json files.\n\nCD_node_template = {\n    'CurrentWorkflowID': str,\n    'ExpectedResponsePath': str,\n    'ResultFilePath': str,\n    'NodeParameters': list,\n    'Version': int,\n    'Tables': list\n}\n\nTo start, these two objects will be defined in the same manner (i.e., they are identical upon initiation). However, eventually, the node_response object will be modified to reflect modified data as well as their structure(s) and location(s).\n\nnode_args = CD_node_template.copy()\nnode_response = CD_node_template.copy()\n\nThe Table(s) defined in the node_args (and node_response) object(s) will have the following format:\n\nCD_table_template = {\n    'TableName': str,\n    'DataFiles': str,\n    'DataFormat': str,\n    'Options': dict,\n    'ColumnDescriptions': list\n}\n\nThe Column(s) defined in the node_args (and node_response) object(s) will have the following format:\n\nCD_column_template = {\n    'ColumnName': str,\n    'ID': str,\n    'DataType': str,\n    'Options': dict\n}\n\n\n\nCreate Objects\nCreate objects (variables) for each node_args, node_response, and CD_tables.\nThe node_args variable had already been created (in the Read Command Line Arguments section) using the data derived from the imported JSON file. Here, we will define (or update) the node_response variable using the data derived from the imported JSON file (and saved in the node_args variable). To start, this variable will be identical to the node_args variable previously referenced.\n\nnode_response = {\n    'CurrentWorkflowID': node_args['CurrentWorkflowID'],\n    'ExpectedResponsePath': node_args['ExpectedResponsePath'],\n    'ResultFilePath': node_args['ResultFilePath'],\n    'NodeParameters': node_args['NodeParameters'],\n    'Version': node_args['Version'],\n    'Tables': node_args['Tables']\n}\n\nDefine (or update) the CD_tables variable using the data derived from the imported JSON file (here, node_args). In this example, only one table was exported (GC EI Compounds); however, the code below can be adjusted to extract data for additional tables.\n\nCD_table_1 = {\n    TableName =  node_args['Tables'][0]['TableName'],\n    DataFiles = node_args['Tables'][0]['DataFile'],\n    DataFormat = node_args['Tables'][0]['DataFormat'],\n    Options = node_args['Tables'][0]['Options'],\n    ColumnDescriptions = node_args['Tables'][0]['ColumnDescriptions']\n}\n\nAlternatively, the user can opt to not update the ColumnDescriptions value with a list of the exported table’s column descriptions. Instead, the user can update the ColumnDescriptions list at a later time to reflect the actual columns referenced in a particular table. In this scenario, the updated CD_table_1 variable would be as follows:\n\nCD_table_1 = {\n    TableName =  node_args['Tables'][0]['TableName'],\n    DataFiles = node_args['Tables'][0]['DataFile'],\n    DataFormat = node_args['Tables'][0]['DataFormat'],\n    Options = node_args['Tables'][0]['Options'],\n    ColumnDescriptions = []\n}"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#load-tables",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#load-tables",
    "title": "Scripting Node Guide",
    "section": "Load Table(s)",
    "text": "Load Table(s)\nRead table(s) exported from Compound Discoverer. Tables are exported as tab-separated text files. In this example, read the contents of the first table’s data file. Define new variable GCEI_Compounds_table and read the GC EI Compounds data into it.\n\nGCEI_Compounds_table = pd.read_table(node_args['Tables'][0]['DataFile'], header=0)"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#data-analysis-and-calculations",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#data-analysis-and-calculations",
    "title": "Scripting Node Guide",
    "section": "Data Analysis and Calculations",
    "text": "Data Analysis and Calculations\nPerform any desired calculations.\nIn this example, we will create a column New CD Column that has the values 1 to number of rows of GC EI Compounds table (the actual values of the column or what calculations we perform are irrelevant for the purposes of the demonstration). We will use this column to demonstrate how to add a column to be imported back into Compound Discoverer.\n\nnew_CD_column = list(range(1, len(GCEI_Compounds_table) + 1))\n\nIn this example, we will create a table New CD Table that computes the Area Mean for each Genuine (Control) set of samples and Suspect (Test) set of samples (again, the contents of the table are irrelevant - perform any desired calculation instead). We will use this table to demonstrate how to add a table to be imported back into Compound Discoverer.\nSubset ‘GCEI_Compounds_table’ indices 0, 3, and 23-28, corresponding to ‘GC EI Compounds ID’, ‘Name’, and individual Area columns for each of the 6 samples (3 Genuine and 3 Suspect).\n\nnew_CD_table = GCEI_Compounds_table.iloc[:, [0, 3, 23, 24, 25, 26, 27, 28]]\nnew_CD_table.iloc[:, 2:8] = new_CD_table.iloc[:, 2:8].apply(pd.to_numeric, errors='coerce')\n\nCompute the Mean values (row-wise) for each set of samples.\n\nnew_CD_table.insert(2, 'Area Mean (Genuine)', new_CD_table.iloc[:, 2:5].mean(axis=1, skipna=True))    # Indices 2, 3, 4 are the Area columns for Genuine samples.\nnew_CD_table.insert(3, 'Area Mean (Suspect)', new_CD_table.iloc[:, 6:9].mean(axis=1, skipna=True))    # Indices 6, 7, 8 are the Area columns for Suspect samples.\n\nRemove the individual Area columns (indices 4:9) since they are no longer necessary.\n\nnew_CD_table = new_CD_table.drop(new_CD_table.columns[4:10], axis=1)\n\nIn this section, we will create two new columns, New CD Table ID and New CD Table WorkflowID. Data pertaining to these files are typically found in the connection table text file and can be read (and assigned) directly by Python. However, in the cases in which these objects were never exported out of Compound Discoverer in the first place (i.e., in situations where the analyst is creating a new table from data generated per the export of only a single table), one can simply create these columns.\nCAUTION: these ID columns are important in how Compound Discoverer indexes the objects of a data table and how it associates them with other objects in other data tables. Therefore, the analyst must follow the proper format and convention to ensure that these two columns’ data are correct.\nDefine New CD Table ID column (that will eventually have the attribute ‘ID’ = ‘ID’ in the JSON file structure). In this example, we will create a new column New CD Table ID.\n\nnew_CD_table['New CD Table ID'] = list(range(1, len(new_CD_table) + 1))\n\nAdditionally, create another column New CD Table WorkflowID. We will assign this column the same value as that of the CurrentWorkflowID.\n\nnew_CD_table['New CD Table WorkflowID'] = node_response['CurrentWorkflowID']\n\nCreate Connection Table data frame, which will eventually be written as a Connection Table text file. This table only requires the ID columns: GC EI Compounds ID, New CD Table ID, and New CD Table WorkflowID. These columns should be maintained in the following order: Original Table ID column, New Table ID column, etc.\n\nconnection_table = new_CD_table[['GC EI Compounds ID', 'New CD Table ID', 'New CD Table WorkflowID']]"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#add-columns",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#add-columns",
    "title": "Scripting Node Guide",
    "section": "Add Column(s)",
    "text": "Add Column(s)\nIn this section, we will outline a procedure of adding a column to both the data table used to perform the calculations in R as well as the node_response (JSON structure) object. Obviously, adding a column to a data frame in Python is quite simple; however, it is of importance to note how the column name is defined and later used in the JSON file structure. The ColumnName value is the column heading that will be displayed in Compound Discoverer upon data import.\nAdd new column to the data table (here, modified_GCEI_Compounds_Table). By defining a column name, we can have both the dataframe and Compound Discoverer (upon import) display New CD Column as the column’s heading (name).\n\nmodified_GCEI_Compounds_Table = GCEI_Compounds_table.copy()\nmodified_GCEI_Compounds_Table['New CD Column'] = new_CD_column\n\nCreate a new column using the JSON structure - Note: the ColumnName value is the same as that of New CD Column defined above (modified_GCEI_Compounds_table$new_CD_column in the table/data frame). In this example, we will override the new_CD_column now that we are done using it (though, this may not be good practice).\nThe following assignment is not necessary, but it serves as a reminder of the CD column’s format.\n\nnew_CD_column = CD_column_template.copy()    # This assignment is unnecessary.\n\nCreate/update the new_CD_column variable.\n\nColumnName: This is the column heading used and displayed by Compound Discoverer.\nID: Unless this is an ID or WorkflowID column, this value is typically an empty string (’’).\nDataType: We will use the type Int (integer), for example demonstration purposes. Adjust as needed (refer to the Manual for possible options).\nOptions: Since we do not have any options in this case, we will set this value to an empty list.\n\n\nnew_CD_column = dict(\n    ColumnName = 'New CD Column',\n    ID = '',\n    DataType = 'Int',\n    Options = {}\n    )\n\nAdd new column to JSON structure (to be used by node_response.json file). Update the node_response variable previously created - append the list of Tables’ ColumnDescriptions by one (for new_CD_column). Repeat this process for each new column as necessary.\n\nnode_response['Tables'][0]['ColumnDescriptions'].append(new_CD_column)"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#add-tables",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#add-tables",
    "title": "Scripting Node Guide",
    "section": "Add Table(s)",
    "text": "Add Table(s)\nCreate a second instance of CD_tables_template, which will be named CD_table_2. This table will be appended to the node_response object and, eventually, to the node_response.json file. This table will serve as the JSON structure file for the newly created table (New CD Table), which will be imported back into Compound Discoverer.\n\nTableName: ‘New CD Table’.\nDataFile: modify the DataFile attribute such that it reflects the NewCDTable.out.txt file.\nDataFormat: ‘CSV’ (files are tab-separated csv text files).\nOptions: update as needed.\nColumnDescriptions: For now, we will set this value to an empty list and update it in the next step.\n\nFirst, get the directory, basename, and extension of the original table’s datafile.\n\ntable_directory, table_filename = os.path.split(node_response['Tables'][0]['DataFile'])\ntable_basename, table_extension = os.path.splitext(table_filename)\n\nCreate a variable that defines the new text file name.\n\nnew_table_file = os.path.join(table_directory, 'NewCDTable.out.txt')\n\nCreate/update ‘CD_table_2’ table structure.\n\nCD_table_2 = dict(\n    TableName = 'New CD Table',\n    DataFile = new_table_file,\n    DataFormat = 'CSV',\n    Options = {},\n    ColumnDescriptions = []\n    )\n\nUpdate the New Table’s CD_table_2 Column Descriptions. It is important that the first ColumnDescription ColumnName value reflect the new table’s ID column (here, New CD Table ID).\nThe ColumnDescription Options list will be updated for the Area columns in order to display values in scientific e notation with 2 decimal places of precision.\n\nCD_table_2['ColumnDescriptions'] = [\n    {\n        'ColumnName': 'New CD Table ID',    # This should reflect the new table's ID column.\n        'ID': 'ID',\n        'DataType': 'Int',\n        'Options': {}\n    },\n    {\n        'ColumnName': 'Name',\n        'ID': '',\n        'DataType': 'String',\n        'Options': {}    # Assuming no options; can be updated later.\n    },\n    {\n        'ColumnName': 'Area Mean (Genuine)',\n        'ID': '',\n        'DataType': 'Float',\n        'Options': {'FormatString': 'e2'}    # Updating the format to display values in scientific e notation and 2 decimal places of precision.\n    },\n    {\n        'ColumnName': 'Area Mean (Suspect)',\n        'ID': '',\n        'DataType': 'Float',\n        'Options': {'FormatString': 'e2'}    # Updating the format to display values in scientific e notation and 2 decimal places of precision.\n    },\n    {\n        'ColumnName': 'New CD Table WorkflowID',\n        'ID': 'WorkflowID',\n        'DataType': 'Int',\n        'Options': {}    # Assuming no options; can be updated later.\n    }\n]\n\nCreate a third instance of CD_tables_template, which will be named CD_table_3. This table will be appended to the node_response object and, eventually, to the node_response.json file. This table will serve as the JSON structure file for the Connection Table between the original table exported out of Compound Discoverer and the newly created table (New CD Table), which will be imported back into Compound Discoverer.\nFirst, create a variable that defines the new text file name. In this example, the resulting file name will terminate as follows: “…/ConsolidatedGCEICompoundItem-NewCDTable.out.txt”.\n\nconnection_table_file = os.path.join(table_directory, table_basename + '-NewCDTable.out.txt')\n\nUpdate the table parameters.\n\nTableName: this should reflect both tables, the original table and the new table connected to it.\nDataFile: use the newly created connection_table_file variable.\nDataFormat: use the format CSVConnectionTable.\nOptions: list the order of the tables (original and new table connected to it).\nColumnDescriptions: we will temporarily use an empty list and update the values of these columns later.\n\n\nCD_table_3 = dict(\n    TableName = 'GC EI Compounds - New CD Table',\n    DataFile = connection_table_file,\n    DataFormat = 'CSVConnectionTable',\n    Options = {\n        'FirstTable': 'GC EI Compounds',\n        'SecondTable': 'New CD Table'\n    },\n    ColumnDescriptions = []\n    )\n\nUpdate Connection Table (CD_table_3) Column Descriptions. Be sure to use the ID columns (previously referenced) and order the columns in a manner consistent with the tables’ connection structure - Original Table followed by the one connected to it.\n\nCD_table_3['ColumnDescriptions'] = [\n        {\n            'ColumnName': 'GC EI Compounds ID',\n            'ID': 'ID',\n            'DataType': 'Int',\n            'Options': {}    # Assuming no options; can be updated later.\n        },\n        {\n            'ColumnName': 'New CD Table ID',\n            'ID': 'ID',\n            'DataType': 'Int',\n            'Options': {}    # Assuming no options; can be updated later.\n        },\n        {\n            'ColumnName': 'New CD Table WorkflowID',\n            'ID': 'WorkflowID',\n            'DataType': 'Int',\n            'Options': {}    # Assuming no options; can be updated later.\n        }\n    ]\n\nUpdate node_response variable to include the newly created tables. In this example, we are creating (appending) two additional tables to node_response (’CD_table_2’ is the structure of the new table (‘new_CD_table’) and ‘CD_table_3’ is the structure of the ‘Connection Table’ (‘connection_table’).\n\nnode_response['Tables'] = node_response['Tables'] + [CD_table_2, CD_table_3]"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#write-files",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#write-files",
    "title": "Scripting Node Guide",
    "section": "Write File(s)",
    "text": "Write File(s)\nData exchange from and to Compound Discoverer typically occurs using temporary files stored in a ‘scratch’ folder. The folder is typically located at the following path: “C:/ProgramData/Thermo/Compound Discoverer 3.3/Scratch”.\nIn this section, we will modify and write text (.out.txt) and JSON (.json) files. The text files will reflect our newly created data (calculations performed using Python) and the JSON files will serve to instruct Compound Discoverer as to the structure of those files, where to find them, and how to read them.\n\nUsing Only Newly Created Column(s)\nWrite newly created results table as a txt (.out.txt) file to temporary (scratch) folder. The ‘.out.txt’ file will be stored in the DataFile field of the tables section of node_response file. Substitute ‘out.txt’ for ‘txt’ in the would be node_response.json file. Write the information for modified_GCEI_Compounds_table.\n\nresult_out_txt = node_response['Tables'][0]['DataFile'].replace('txt', 'out.txt')\n\nWrite newly created table results to file. Note: write files as tab-separated (CSV) text files.\n\nmodified_GCEI_Compounds_Table.to_csv(result_out_txt, sep='\\t', index=False, encoding='utf-8')\n\nUpdate node_response.json table DataFile path (with ‘.out.txt’).\n\nnode_response['Tables'][0]['DataFile'] = result_out_txt\n\nWrite node_response.json file. Define ExpectedResponsePath location.\n\njson_out_file = node_response['ExpectedResponsePath']\n\nConvert/save node_response to JSON structure.\n\ntry:\n    with open(json_out_file, mode='w') as f:\n        json.dump(node_response, f)\n    print('Successfully saved node_response.json file!')\n\nexcept Exception as e:\n    print('Failed to save node_response.json file: ' + str(e))\n    print(traceback.format_exc())\n    exit(1)\n\n\n\nUsing Only Newly Created Table(s)\nWrite newly created table results to file. Note: write files as tab-separated (CSV) text files.\n\nnew_CD_table.to_csv(node_response['Tables'][1]['DataFile'], sep='\\t', index=False, encoding='utf-8')\n\nWrite newly created table to Connection Table portion of the file.\n\nconnection_table.to_csv(node_response['Tables'][2]['DataFile'], sep='\\t', index=False, encoding='utf-8')\n\nWrite node_response.json file. Define ExpectedResponsePath location.\n\njson_out_file = node_response['ExpectedResponsePath']\n\nConvert/save node_response to JSON structure.\n\ntry:\n    with open(json_out_file, mode='w') as f:\n        json.dump(node_response, f)\n    print('Successfully saved node_response.json file!')\n\nexcept Exception as e:\n    print('Failed to save node_response.json file: ' + str(e))\n    print(traceback.format_exc())\n    exit(1)\n\n\n\nUsing Combined Newly Created Column and Table(s)\nWrite newly created results table as a txt (.out.txt) file to temporary (scratch) folder. The ‘.out.txt’ file will be stored in the DataFile field of the tables section of node_response file. Substitute ‘out.txt’ for ‘txt’ in the would be node_response.json file. Write the information for modified_GCEI_Compounds_table.\n\nresult_out_txt = node_response['Tables'][0]['DataFile'].replace('txt', 'out.txt')\n\nWrite newly created table results to file. Note: write files as tab-separated (CSV) text files.\n\nmodified_GCEI_Compounds_Table.to_csv(result_out_txt, sep='\\t', index=False, encoding='utf-8')\n\nUpdate node_response.json table DataFile path (with ‘.out.txt’).\n\nnode_response['Tables'][0]['DataFile'] = result_out_txt\n\nWrite newly created table results to file. Note: write files as tab-separated (CSV) text files.\n\nnew_CD_table.to_csv(node_response['Tables'][1]['DataFile'], sep='\\t', index=False, encoding='utf-8')\n\nWrite newly created table to Connection Table portion of the file.\n\nconnection_table.to_csv(node_response['Tables'][2]['DataFile'], sep='\\t', index=False, encoding='utf-8')\n\nWrite node_response.json file. Define ExpectedResponsePath location.\n\njson_out_file = node_response['ExpectedResponsePath']\n\nConvert/save node_response to JSON structure.\n\ntry:\n    with open(json_out_file, mode='w') as f:\n        json.dump(node_response, f)\n    print('Successfully saved node_response.json file!')\n\nexcept Exception as e:\n    print('Failed to save node_response.json file: ' + str(e))\n    print(traceback.format_exc())\n    exit(1)"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#session-information",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#session-information",
    "title": "Scripting Node Guide",
    "section": "Session Information",
    "text": "Session Information\n\nprint(sys.version)\n# '3.13.0 | packaged by Anaconda, Inc. | (main, Oct  7 2024, 21:21:52) [MSC v.1929 64 bit (AMD64)]'\n\nprint(json.__version__)\n# '2.0.9'\n\nprint(pd.__version__)\n# '2.2.3'"
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#additional-resources",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#additional-resources",
    "title": "Scripting Node Guide",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nProteome Discoverer 3.1 - User Guide - Scripting Node Overview.\nMyCompoundDiscoverer.com - Scripting Node.\nCompound Discoverer Tutorials."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#disclaimer",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#disclaimer",
    "title": "Scripting Node Guide",
    "section": "Disclaimer",
    "text": "Disclaimer\nWhile the author may be affiliated with Thermo Fisher Scientific at the time of this writing, his work reflects his own views, opinions, guidelines, and/or recommendations. In no way, shape, or form does the author’s work reflect the views, opinions, or recommendations of Thermo Fisher Scientific, Inc., its affiliates, or its employees. Further, the author is not part of the Compound Discoverer (or Proteome Discoverer) software development or management teams and has no additional insight as to the underlying mechanism of the software, its code, or any other details beyond that of the typical software user. As such, in no way, shape, or form does the author’s work reflect the views, opinions, or recommendations of the Compound Discoverer (or Proteome Discoverer) software development and/or management teams.\nThe work presented in this guide and scripts has been tested with limited scope using Compound Discoverer 3.3 SP3 (CD 3.3.3) primarily using the GC Workflows. The work is only meant to serve as a guide rather than best practice recommendations. The code and scripts have not been validated or tested extensively; they are intended for Research Use Only and not for use in diagnostic procedures, regulated environments, or the like.\nIn an effort to keep the Guide clear and simple, the author attempted to complete this work while strictly using base Python and employing the minimal amount of features and functionality afforded by the base distribution. In situations where that was not possible, the author attempted to maintain the level of clarity and simplicity to the best of his abilities. It is quite possible that more efficient, effective, and flexible scripts, processes, or workflows can be accomplished with the use of additional packages or tools.\nIt is assumed that the user is already familiar with and/or has had formal training pertaining to the Compound Discoverer software. In no way, shape, or form does this guide constitute a form of training or a replacement to formal training. Furthermore, in no way, shape, or form does this guide (ore repository) constitute or represent a medium through which any type of training or troubleshooting should be sought, expected, or administered.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nBY DOWNLOADING OR USING ANY SOFTWARE, SCRIPTS, TEMPLATES, DOCUMENTATION AND/OR OTHER MATERIALS (COLLECTIVELY “MATERIALS”), YOU AND ANY COMPANY OR INSTITUTION YOU REPRESENT (COLLECTIVELY “YOU”) ACKNOWLEDGE AND AGREE AS FOLLOWS: (1) THE MATERIALS ARE PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESSED OR IMPLIED, AND (2) THERMO FISHER SCIENTIFIC INC., ITS AFFILIATES AND EMPLOYEES WILL NOT BE RESPONSIBLE FOR ANY DAMAGES ARISING FROM YOUR USE OF THE MATERIALS, INCLUDING BUT NOT LIMITED TO DAMAGES ASSOCIATED WITH LOSS OR CORRUPTION OF DATA, INACCURATE RESULTS, AND/OR DIMINISHED INSTRUMENT PERFORMANCE."
  },
  {
    "objectID": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#license",
    "href": "Compound-Discoverer/Scripting-Node/Scripting-Node-Guide-for-Python.html#license",
    "title": "Scripting Node Guide",
    "section": "License",
    "text": "License\nMIT License\nCopyright (c) 2025 Ahmad Alamiri\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  }
]